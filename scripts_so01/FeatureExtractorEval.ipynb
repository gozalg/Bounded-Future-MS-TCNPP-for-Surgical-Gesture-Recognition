{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-gozal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/home/gabrielg/.netrc\n"
     ]
    }
   ],
   "source": [
    "#----------------- Python Libraries Imports -----------------#\n",
    "# Python Standard Library\n",
    "\n",
    "# Third-party libraries\n",
    "\n",
    "#------------------ Bounded Future Imports ------------------#\n",
    "from FeatureExtractorTrainer import *\n",
    "#------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overall:\n",
    "    def __init__(self):\n",
    "        self.acc_mean = None\n",
    "        self.edit_mean = None\n",
    "        self.avg_f1_mean = None\n",
    "        self.f1_10_mean = None\n",
    "        self.f1_25_mean = None\n",
    "        self.f1_50_mean = None\n",
    "\n",
    "def test(model, test_loaders, device_gpu, device_cpu, num_class, gesture_ids, output_folder=None, epoch=None, upload=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        overall_acc = []\n",
    "        overall_avg_f1 = []\n",
    "        overall_edit = []\n",
    "        overall_f1_10 = []\n",
    "        overall_f1_25 = []\n",
    "        overall_f1_50 = []\n",
    "\n",
    "        overall = Overall()  # Initialize overall as an object of Overall class\n",
    "\n",
    "        for test_loader in test_loaders:\n",
    "            P = np.array([], dtype=np.int64)\n",
    "            Y = np.array([], dtype=np.int64)\n",
    "\n",
    "            train_loader_iter = iter(test_loader)\n",
    "            while True:\n",
    "                try:\n",
    "                    (data, target) = next(train_loader_iter)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except (FileNotFoundError, PIL.UnidentifiedImageError) as e:\n",
    "                    print(e)\n",
    "\n",
    "                # for i, batch in enumerate(test_loader):\n",
    "                # data, target = batch\n",
    "                Y = np.append(Y, target.numpy())\n",
    "                data = data.to(device_gpu)\n",
    "                output = model(data)\n",
    "\n",
    "                if len(output.shape) > 2:\n",
    "                    output = output[:, :, -1]  # consider only final prediction\n",
    "                predicted = torch.nn.Softmax(dim=1)(output)\n",
    "                _, predicted = torch.max(predicted, 1)\n",
    "                P = np.append(P, predicted.to(device_cpu).numpy())\n",
    "            acc = accuracy(P, Y)\n",
    "\n",
    "            mean_avg_f1, avg_precision, avg_recall, avg_f1 = average_F1(P, Y, n_classes=num_class)\n",
    "            # if upload:\n",
    "            # avg_precision_table = wandb.Table(data=avg_precision, columns=gestures_SU)\n",
    "            # wandb.log({\"my_custom_plot_id\": wandb.plot.line(avg_precision_table, \"x\", \"avg_precision\",\n",
    "            #                                                 title=\"Custom Y vs X Line Plot\")})\n",
    "\n",
    "            avg_precision_ = np.array(avg_precision)\n",
    "            avg_recall_ = np.array(avg_recall)\n",
    "            avg_f1_ = np.array(avg_f1)\n",
    "            gesture_ids_ = gesture_ids.copy() + [\"mean\"]\n",
    "            avg_precision.append(np.mean(avg_precision_[(avg_precision_) != np.array(None)]))\n",
    "            avg_recall.append(np.mean(avg_recall_[(avg_recall_) != np.array(None)]))\n",
    "            avg_f1.append(np.mean(avg_f1_[(avg_f1_) != np.array(None)]))\n",
    "            df = pd.DataFrame(list(zip(gesture_ids_, avg_precision, avg_recall, avg_f1)),\n",
    "                                columns=['gesture_ids', 'avg_precision', 'avg_recall', 'avg_f1'])\n",
    "            if output_folder:\n",
    "                log(df, output_folder)\n",
    "            edit = edit_score(P, Y)\n",
    "            f1_10 = overlap_f1(P, Y, n_classes=num_class, overlap=0.1)\n",
    "            f1_25 = overlap_f1(P, Y, n_classes=num_class, overlap=0.25)\n",
    "            f1_50 = overlap_f1(P, Y, n_classes=num_class, overlap=0.5)\n",
    "            if output_folder:\n",
    "                log(\"Trial {}:\\tAcc - {:.3f} Avg_F1 - {:.3f} Edit - {:.3f} F1_10 {:.3f} F1_25 {:.3f} F1_50 {:.3f}\"\n",
    "                    .format(test_loader.dataset.video_id, acc, mean_avg_f1, edit, f1_10, f1_25, f1_50), output_folder)\n",
    "\n",
    "            overall_acc.append(acc)\n",
    "            overall_avg_f1.append(mean_avg_f1)\n",
    "            overall_edit.append(edit)\n",
    "            overall_f1_10.append(f1_10)\n",
    "            overall_f1_25.append(f1_25)\n",
    "            overall_f1_50.append(f1_50)\n",
    "        if output_folder:\n",
    "            log(\"Overall: Acc - {:.3f} Avg_F1 - {:.3f} Edit - {:.3f} F1_10 {:.3f} F1_25 {:.3f} F1_50 {:.3f}\".format(\n",
    "                np.mean(overall_acc), np.mean(overall_avg_f1), np.mean(overall_edit),\n",
    "                np.mean(overall_f1_10), np.mean(overall_f1_25), np.mean(overall_f1_50)\n",
    "            ), output_folder)\n",
    "\n",
    "        \n",
    "        if upload:\n",
    "            wandb.log({'validation accuracy': np.mean(overall_acc), 'Avg_F1': np.mean(overall_avg_f1), \n",
    "                        'Edit': np.mean(overall_edit), \"F1_10\": np.mean(overall_f1_10), \"F1_25\": np.mean(overall_f1_25),\n",
    "                        \"F1_50\": np.mean(overall_f1_50)}, step=epoch)\n",
    "        overall.acc_mean    = np.mean(overall_acc)\n",
    "        overall.edit_mean   = np.mean(overall_edit)\n",
    "        overall.avg_f1_mean = np.mean(overall_avg_f1)\n",
    "        overall.f1_10_mean  = np.mean(overall_f1_10)\n",
    "        overall.f1_25_mean  = np.mean(overall_f1_25)\n",
    "        overall.f1_50_mean  = np.mean(overall_f1_50)\n",
    "\n",
    "    return overall\n",
    "\n",
    "def no_none_collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 60 # 30 for JIGSAWS, 60 for SAR_RARP50\n",
    "labels_Hz = 10 # 30 for JIGSAWS, 10 for SAR_RARP50\n",
    "# args for testing the model\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.gpu_id = 0\n",
    "        self.arch = '2D-EfficientNetV2-m',\n",
    "        self.video_lists_dir = \"/data/home/gabrielg/Bounded_Future_from_GIT/data/SAR_RARP50/Splits/\"\n",
    "        self.data_path = \"/data/home/gabrielg/Bounded_Future_from_GIT/data/SAR_RARP50/frames\"\n",
    "        self.transcriptions_dir = \"/data/home/gabrielg/Bounded_Future_from_GIT/data/SAR_RARP50/transcriptions\"\n",
    "        self.model_path = \"/data/home/gabrielg/Bounded_Future_from_GIT/output/feature_extractor\"\n",
    "        self.dataset = 'SAR_RARP50' #'JIGSAWS'  # or MultiBypass140\n",
    "        self.num_classes = 10  # 10 for JIGSAWS, 8 for SAR_RARP50, n for MultiBypass\n",
    "        self.eval_scheme = 'LOUO'  # LOUO or LOSO\n",
    "        self.task = 'Suturing' # for JIGSAWS, 'None' for SAR_RARP50\n",
    "        self.split = 0\n",
    "        self.snippet_length = 1\n",
    "        self.val_sampling_step = fps // labels_Hz\n",
    "        self.image_tmpl = '{:09d}.png' #'img_{:05d}.jpg' for JIGSAWS, '{:09d}.png' for SAR_RARP50\n",
    "        self.video_suffix = 'None' # '_capture2' for JIGSAWS, 'None' for SAR_RARP50\n",
    "        self.input_size = 224\n",
    "        self.batch_size = 32\n",
    "        self.workers = 64\n",
    "    def next_split(self):\n",
    "        self.split += 1\n",
    "        return self.split\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/gabrielg/miniconda3/envs/BoundedFuture/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading images from video video_41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/gabrielg/miniconda3/envs/BoundedFuture/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 28, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading images from video video_42...\n",
      "Preloading images from video video_43...\n",
      "Preloading images from video video_44...\n",
      "Preloading images from video video_45...\n",
      "Preloading images from video video_46...\n",
      "Preloading images from video video_47...\n",
      "Preloading images from video video_48...\n",
      "Preloading images from video video_49...\n",
      "Preloading images from video video_50...\n"
     ]
    }
   ],
   "source": [
    "# ===== load data =====\n",
    "gesture_ids = get_gestures(args.dataset, args.task)\n",
    "args.eval_batch_size = 2 * args.batch_size\n",
    "normalize = GroupNormalize(INPUT_MEAN, INPUT_STD)\n",
    "\n",
    "if args.dataset == \"JIGSAWS\":\n",
    "    splits = get_splits(args.dataset, args.eval_scheme, args.task)\n",
    "    _, test_list = train_val_split(splits, args.split)\n",
    "    lists_dir = os.path.join(args.video_lists_dir, args.eval_scheme)\n",
    "elif args.dataset == \"SAR_RARP50\":\n",
    "    test_list = {'data_test.csv'}\n",
    "    lists_dir = args.video_lists_dir\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "val_augmentation = torchvision.transforms.Compose([GroupScale(args.input_size), GroupCenterCrop(args.input_size)])\n",
    "test_lists = list(map(lambda x: os.path.join(lists_dir, x), test_list))\n",
    "\n",
    "test_videos = list()\n",
    "for list_file in test_lists:\n",
    "    test_videos.extend([(x.strip().split(',')[0], x.strip().split(',')[1]) for x in open(list_file)])\n",
    "test_loaders = list()\n",
    "# in JIGSAWS there is no validation, so each split the test set changes\n",
    "if (args.dataset == \"JIGSAWS\") or (args.dataset == \"SAR_RARP50\" and args.split==0):\n",
    "    for video in test_videos:\n",
    "        data_set = Sequential2DTestGestureDataSet(dataset=args.dataset, root_path=args.data_path, sar_rarp50_sub_dir='test', video_id=video[0], frame_count=video[1],\n",
    "                                                    transcriptions_dir=args.transcriptions_dir, gesture_ids=gesture_ids,\n",
    "                                                    snippet_length=args.snippet_length,\n",
    "                                                    sampling_step=args.val_sampling_step,\n",
    "                                                    image_tmpl=args.image_tmpl,\n",
    "                                                    video_suffix=args.video_suffix,\n",
    "                                                    normalize=normalize, resize=args.input_size,\n",
    "                                                    transform=val_augmentation)  # augmentation are off\n",
    "        test_loaders.append(torch.utils.data.DataLoader(data_set, batch_size=args.eval_batch_size,\n",
    "                                                        shuffle=False, num_workers=args.workers,\n",
    "                                                        collate_fn=no_none_collate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(  args.arch[0], \n",
    "                    num_classes=args.num_classes,\n",
    "                    add_layer_param_num=0,\n",
    "                    add_certainty_pred=0,\n",
    "                    input_shape=0,\n",
    "                    embedding_shape=0,\n",
    "                    vae_intermediate_size=None\n",
    "                )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model weights from output folder\n",
    "# best_model_loc = f\"self.model_path/{args.dataset}/{args.arch[0]}/{args.eval_scheme}/{args.split}/best_{args.split}.pth\"\n",
    "model_loc = f\"{args.model_path}/{args.dataset}/{args.arch[0]}/{args.eval_scheme}/{args.split}/model_99.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EfficientNet:\n\tsize mismatch for added_fc.weight: copying a param with shape torch.Size([8, 1280]) from checkpoint, the shape in current model is torch.Size([10, 1280]).\n\tsize mismatch for added_fc.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_loc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/BoundedFuture/lib/python3.9/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EfficientNet:\n\tsize mismatch for added_fc.weight: copying a param with shape torch.Size([8, 1280]) from checkpoint, the shape in current model is torch.Size([10, 1280]).\n\tsize mismatch for added_fc.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/gabrielg/miniconda3/envs/BoundedFuture/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 28, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/data/home/gabrielg/Bounded_Future_from_GIT/utils/metrics.py:275: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  F1 = 2 * (precision*recall) / (precision+recall)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "device_gpu = torch.device(f\"cuda:{args.gpu_id}\")\n",
    "model = model.to(device_gpu)\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "overall = test(model, test_loaders, device_gpu, device_cpu, args.num_classes, gesture_ids, output_folder=None, epoch=None, upload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mSplit \u001b[93m0\u001b[94m:\u001b[0m\n",
      "\u001b[94m\tTest Acc: \u001b[93m\t77.929\u001b[0m\n",
      "\u001b[94m\tTest Macro F1: \u001b[93m\t69.902\u001b[0m\n",
      "\u001b[94m\tTest F1@10: \u001b[93m\t27.395\u001b[0m\n",
      "\u001b[94m\tTest F1@25: \u001b[93m\t26.450\u001b[0m\n",
      "\u001b[94m\tTest F1@50: \u001b[93m\t20.417\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_acc        = overall.acc_mean\n",
    "test_edit       = overall.edit_mean\n",
    "test_macro_f1   = overall.avg_f1_mean\n",
    "test_f1_10      = overall.f1_10_mean\n",
    "test_f1_25      = overall.f1_25_mean\n",
    "test_f1_50      = overall.f1_50_mean\n",
    "\n",
    "# print in blue text and in yellow results including args.split\n",
    "print(\"\\033[94m\" + \"Split \" + \"\\033[93m\" + f\"{args.split}\" + \"\\033[94m\" + \":\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest Acc: \" + \"\\033[93m\" + f\"\\t{test_acc:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest Macro F1: \" + \"\\033[93m\" + f\"\\t{test_macro_f1:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest Edit: \" + \"\\033[93m\" + f\"\\t{test_edit:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@10: \" + \"\\033[93m\" + f\"\\t{test_f1_10:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@25: \" + \"\\033[93m\" + f\"\\t{test_f1_25:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@50: \" + \"\\033[93m\" + f\"\\t{test_f1_50:.3f}\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep results in csv file split is the row reference and the columns are the metrics\n",
    "results = pd.DataFrame(columns=[\"split\", \"test_acc\", \"test_macro_f1\", \"test_edit\",  \"test_f1_10\", \"test_f1_25\", \"test_f1_50\"])\n",
    "results.loc[0] = [args.split, test_acc, test_macro_f1, test_edit, test_f1_10, test_f1_25, test_f1_50]\n",
    "results.to_csv(f\"self.model_path/{args.dataset}/{args.arch[0]}/{args.eval_scheme}/eval_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BoundedFuture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
