{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-gozal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/home/gabrielg/.netrc\n"
     ]
    }
   ],
   "source": [
    "#----------------- Python Libraries Imports -----------------#\n",
    "# Python Standard Library\n",
    "\n",
    "# Third-party libraries\n",
    "\n",
    "#------------------ Bounded Future Imports ------------------#\n",
    "from FeatureExtractorTrainer import *\n",
    "#------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overall:\n",
    "    def __init__(self):\n",
    "        self.acc_mean = None\n",
    "        self.avg_f1_mean = None\n",
    "        self.f1_10_mean = None\n",
    "        self.f1_25_mean = None\n",
    "        self.f1_50_mean = None\n",
    "\n",
    "def test(model, val_loaders, device_gpu, device_cpu, num_class, gesture_ids, output_folder=None, epoch=None, upload=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        overall_acc = []\n",
    "        overall_avg_f1 = []\n",
    "        overall_edit = []\n",
    "        overall_f1_10 = []\n",
    "        overall_f1_25 = []\n",
    "        overall_f1_50 = []\n",
    "\n",
    "        overall = Overall()  # Initialize overall as an object of Overall class\n",
    "\n",
    "        for val_loader in val_loaders:\n",
    "            P = np.array([], dtype=np.int64)\n",
    "            Y = np.array([], dtype=np.int64)\n",
    "\n",
    "            train_loader_iter = iter(val_loader)\n",
    "            while True:\n",
    "                try:\n",
    "                    (data, target) = next(train_loader_iter)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except (FileNotFoundError, PIL.UnidentifiedImageError) as e:\n",
    "                    print(e)\n",
    "\n",
    "                # for i, batch in enumerate(val_loader):\n",
    "                # data, target = batch\n",
    "                Y = np.append(Y, target.numpy())\n",
    "                data = data.to(device_gpu)\n",
    "                output = model(data)\n",
    "\n",
    "                if len(output.shape) > 2:\n",
    "                    output = output[:, :, -1]  # consider only final prediction\n",
    "                predicted = torch.nn.Softmax(dim=1)(output)\n",
    "                _, predicted = torch.max(predicted, 1)\n",
    "                P = np.append(P, predicted.to(device_cpu).numpy())\n",
    "            acc = accuracy(P, Y)\n",
    "\n",
    "            mean_avg_f1, avg_precision, avg_recall, avg_f1 = average_F1(P, Y, n_classes=num_class)\n",
    "            # if upload:\n",
    "            # avg_precision_table = wandb.Table(data=avg_precision, columns=gestures_SU)\n",
    "            # wandb.log({\"my_custom_plot_id\": wandb.plot.line(avg_precision_table, \"x\", \"avg_precision\",\n",
    "            #                                                 title=\"Custom Y vs X Line Plot\")})\n",
    "\n",
    "            avg_precision_ = np.array(avg_precision)\n",
    "            avg_recall_ = np.array(avg_recall)\n",
    "            avg_f1_ = np.array(avg_f1)\n",
    "            gesture_ids_ = gesture_ids.copy() + [\"mean\"]\n",
    "            avg_precision.append(np.mean(avg_precision_[(avg_precision_) != np.array(None)]))\n",
    "            avg_recall.append(np.mean(avg_recall_[(avg_recall_) != np.array(None)]))\n",
    "            avg_f1.append(np.mean(avg_f1_[(avg_f1_) != np.array(None)]))\n",
    "            df = pd.DataFrame(list(zip(gesture_ids_, avg_precision, avg_recall, avg_f1)),\n",
    "                                columns=['gesture_ids', 'avg_precision', 'avg_recall', 'avg_f1'])\n",
    "            if output_folder:\n",
    "                log(df, output_folder)\n",
    "            edit = edit_score(P, Y)\n",
    "            f1_10 = overlap_f1(P, Y, n_classes=num_class, overlap=0.1)\n",
    "            f1_25 = overlap_f1(P, Y, n_classes=num_class, overlap=0.25)\n",
    "            f1_50 = overlap_f1(P, Y, n_classes=num_class, overlap=0.5)\n",
    "            if output_folder:\n",
    "                log(\"Trial {}:\\tAcc - {:.3f} Avg_F1 - {:.3f} Edit - {:.3f} F1_10 {:.3f} F1_25 {:.3f} F1_50 {:.3f}\"\n",
    "                    .format(val_loader.dataset.video_id, acc, mean_avg_f1, edit, f1_10, f1_25, f1_50), output_folder)\n",
    "\n",
    "            overall_acc.append(acc)\n",
    "            overall_avg_f1.append(mean_avg_f1)\n",
    "            overall_edit.append(edit)\n",
    "            overall_f1_10.append(f1_10)\n",
    "            overall_f1_25.append(f1_25)\n",
    "            overall_f1_50.append(f1_50)\n",
    "        if output_folder:\n",
    "            log(\"Overall: Acc - {:.3f} Avg_F1 - {:.3f} Edit - {:.3f} F1_10 {:.3f} F1_25 {:.3f} F1_50 {:.3f}\".format(\n",
    "                np.mean(overall_acc), np.mean(overall_avg_f1), np.mean(overall_edit),\n",
    "                np.mean(overall_f1_10), np.mean(overall_f1_25), np.mean(overall_f1_50)\n",
    "            ), output_folder)\n",
    "\n",
    "        \n",
    "        if upload:\n",
    "            wandb.log({'validation accuracy': np.mean(overall_acc), 'Avg_F1': np.mean(overall_avg_f1), \n",
    "                        'Edit': np.mean(overall_edit), \"F1_10\": np.mean(overall_f1_10), \"F1_25\": np.mean(overall_f1_25),\n",
    "                        \"F1_50\": np.mean(overall_f1_50)}, step=epoch)\n",
    "        overall.acc_mean    = np.mean(overall_acc)\n",
    "        overall.avg_f1_mean = np.mean(overall_avg_f1)\n",
    "        overall.f1_10_mean  = np.mean(overall_f1_10)\n",
    "        overall.f1_25_mean  = np.mean(overall_f1_25)\n",
    "        overall.f1_50_mean  = np.mean(overall_f1_50)\n",
    "\n",
    "    return overall\n",
    "\n",
    "def no_none_collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for testing the model\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.gpu_id = 0\n",
    "        self.arch = '2D-EfficientNetV2-m',\n",
    "        self.video_lists_dir = \"/data/home/gabrielg/BoundedFuture++/Bounded_Future_from_GIT/data/JIGSAWS/Splits/Suturing\"\n",
    "        self.data_path = \"/data/home/gabrielg/BoundedFuture++/Bounded_Future_from_GIT/data/JIGSAWS/Suturing/frames\"\n",
    "        self.transcriptions_dir = \"/data/home/gabrielg/BoundedFuture++/Bounded_Future_from_GIT/data/JIGSAWS/Suturing/transcriptions\"\n",
    "        self.dataset = 'JIGSAWS'  # RARP50 or MultiBypass140\n",
    "        self.num_classes = 10  # 10 for JIGSAWS, n for RARP50, n for MultiBypass\n",
    "        self.eval_scheme = 'LOUO'  # LOUO or LOSO\n",
    "        self.task = 'Suturing'\n",
    "        self.split = 0\n",
    "        self.snippet_length = 1\n",
    "        self.val_sampling_step = 1\n",
    "        self.image_tmpl = 'img_{:05d}.jpg'\n",
    "        self.video_suffix = '_capture2'\n",
    "        self.input_size = 224\n",
    "        self.batch_size = 32\n",
    "        self.workers = 4\n",
    "    def next_split(self):\n",
    "        self.split += 1\n",
    "        return self.split\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading images from video Suturing_B001...\n",
      "Preloading images from video Suturing_B002...\n",
      "Preloading images from video Suturing_B003...\n",
      "Preloading images from video Suturing_B004...\n",
      "Preloading images from video Suturing_B005...\n"
     ]
    }
   ],
   "source": [
    "# ===== load data =====\n",
    "gesture_ids = get_gestures(args.dataset, args.task)\n",
    "args.eval_batch_size = 2 * args.batch_size\n",
    "normalize = GroupNormalize(INPUT_MEAN, INPUT_STD)\n",
    "\n",
    "splits = get_splits(args.dataset, args.eval_scheme, args.task)\n",
    "_, val_list = train_val_split(splits, args.split)\n",
    "\n",
    "val_augmentation = torchvision.transforms.Compose([GroupScale(args.input_size), GroupCenterCrop(args.input_size)])\n",
    "\n",
    "lists_dir = os.path.join(args.video_lists_dir, args.eval_scheme)\n",
    "\n",
    "val_lists = list(map(lambda x: os.path.join(lists_dir, x), val_list))\n",
    "\n",
    "val_videos = list()\n",
    "for list_file in val_lists:\n",
    "    val_videos.extend([(x.strip().split(',')[0], x.strip().split(',')[1]) for x in open(list_file)])\n",
    "val_loaders = list()\n",
    "\n",
    "for video in val_videos:\n",
    "    data_set = Sequential2DTestGestureDataSet(root_path=args.data_path, video_id=video[0], frame_count=video[1],\n",
    "                                                transcriptions_dir=args.transcriptions_dir, gesture_ids=gesture_ids,\n",
    "                                                snippet_length=args.snippet_length,\n",
    "                                                sampling_step=args.val_sampling_step,\n",
    "                                                image_tmpl=args.image_tmpl,\n",
    "                                                video_suffix=args.video_suffix,\n",
    "                                                normalize=normalize, resize=args.input_size,\n",
    "                                                transform=val_augmentation)  # augmentation are off\n",
    "    val_loaders.append(torch.utils.data.DataLoader(data_set, batch_size=args.eval_batch_size,\n",
    "                                                    shuffle=False, num_workers=args.workers,\n",
    "                                                    collate_fn=no_none_collate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(  args.arch[0], \n",
    "                    num_classes=args.num_classes,\n",
    "                    add_layer_param_num=0,\n",
    "                    add_certainty_pred=0,\n",
    "                    input_shape=0,\n",
    "                    embedding_shape=0,\n",
    "                    vae_intermediate_size=None\n",
    "                )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model weights from output folder\n",
    "best_model_loc = f\"/data/home/gabrielg/BoundedFuture++/Bounded_Future_from_GIT/output/feature_extractor/{args.dataset}/{args.arch[0]}/{args.eval_scheme}/{args.split}/best_{args.split}.pth\"\n",
    "model.load_state_dict(torch.load(best_model_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device_gpu = torch.device(f\"cuda:{args.gpu_id}\")\n",
    "model = model.to(device_gpu)\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "# val_loaders\n",
    "splits = get_splits(args.dataset, args.eval_scheme, args.task)\n",
    "_, val_list = train_val_split(splits, args.split)\n",
    "\n",
    "overall = test(model, val_loaders, device_gpu, device_cpu, args.num_classes, gesture_ids, output_folder=None, epoch=None, upload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mSplit \u001b[93m0\u001b[94m:\u001b[0m\n",
      "\u001b[94m\tTest Acc: \u001b[93m\t64.603\u001b[0m\n",
      "\u001b[94m\tTest Macro F1: \u001b[93m\t58.169\u001b[0m\n",
      "\u001b[94m\tTest F1@10: \u001b[93m\t14.457\u001b[0m\n",
      "\u001b[94m\tTest F1@25: \u001b[93m\t12.660\u001b[0m\n",
      "\u001b[94m\tTest F1@50: \u001b[93m\t6.517\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_acc        = overall.acc_mean\n",
    "test_macro_f1   = overall.avg_f1_mean\n",
    "test_f1_10      = overall.f1_10_mean\n",
    "test_f1_25      = overall.f1_25_mean\n",
    "test_f1_50      = overall.f1_50_mean\n",
    "# print in blue text and in yellow results including args.split\n",
    "print(\"\\033[94m\" + \"Split \" + \"\\033[93m\" + f\"{args.split}\" + \"\\033[94m\" + \":\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest Acc: \" + \"\\033[93m\" + f\"\\t{test_acc:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest Macro F1: \" + \"\\033[93m\" + f\"\\t{test_macro_f1:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@10: \" + \"\\033[93m\" + f\"\\t{test_f1_10:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@25: \" + \"\\033[93m\" + f\"\\t{test_f1_25:.3f}\" + \"\\033[0m\")\n",
    "print(\"\\033[94m\" + \"\\tTest F1@50: \" + \"\\033[93m\" + f\"\\t{test_f1_50:.3f}\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep results in csv file split is the row reference and the columns are the metrics\n",
    "results = pd.DataFrame(columns=[\"split\", \"test_acc\", \"test_macro_f1\", \"test_f1_10\", \"test_f1_25\", \"test_f1_50\"])\n",
    "results.loc[0] = [args.split, test_acc, test_macro_f1, test_f1_10, test_f1_25, test_f1_50]\n",
    "results.to_csv(f\"/data/home/gabrielg/BoundedFuture++/Bounded_Future_from_GIT/output/feature_extractor/{args.dataset}/{args.arch[0]}/{args.eval_scheme}/results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BoundedFuture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
